{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김치찌개  파일 길이 :  20\n",
      "김치찌개  :  ./dataset_2/train/김치찌개/3 copy 3.jpeg\n",
      "라면  파일 길이 :  20\n",
      "라면  :  ./dataset_2/train/라면/3 copy 3.jpeg\n",
      "양념게장  파일 길이 :  20\n",
      "양념게장  :  ./dataset_2/train/양념게장/3 copy 3.jpeg\n",
      "제육볶음  파일 길이 :  20\n",
      "제육볶음  :  ./dataset_2/train/제육볶음/3 copy 3.jpeg\n",
      "된장찌개  파일 길이 :  20\n",
      "된장찌개  :  ./dataset_2/train/된장찌개/3 copy 3.jpeg\n",
      "돈까스  파일 길이 :  20\n",
      "돈까스  :  ./dataset_2/train/돈까스/3 copy 3.jpeg\n",
      "김치볶음밥  파일 길이 :  20\n",
      "김치볶음밥  :  ./dataset_2/train/김치볶음밥/3 copy 3.jpeg\n",
      "비빔밥  파일 길이 :  20\n",
      "비빔밥  :  ./dataset_2/train/비빔밥/3 copy 3.jpeg\n",
      "삼겹살  파일 길이 :  20\n",
      "삼겹살  :  ./dataset_2/train/삼겹살/3 copy 3.jpeg\n",
      "스파게티  파일 길이 :  20\n",
      "스파게티  :  ./dataset_2/train/스파게티/3 copy 3.jpeg\n",
      "ok 200\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"./dataset_2/train\"\n",
    "\n",
    "categories = [\"김치찌개\", \"라면\", \"양념게장\", \"제육볶음\",\"된장찌개\",\n",
    "             \"돈까스\",\"김치볶음밥\",\"비빔밥\",\"삼겹살\",\"스파게티\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpeg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#1 0 0 0 이면 airplanes\n",
    "#0 1 0 0 이면 buddha 이런식\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./upgrade_numpy_data/multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 64, 64, 3)\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./upgrade_numpy_data/multi_image_data.npy',allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"김치찌개\", \"라면\", \"양념게장\", \"제육볶음\",\"된장찌개\",\n",
    "             \"돈까스\",\"김치볶음밥\",\"비빔밥\",\"삼겹살\",\"스파게티\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_dir = './upgrade_model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/upgrade_multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 4,216,522\n",
      "Trainable params: 4,216,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 2.7875 - accuracy: 0.0863 - val_loss: 2.3115 - val_accuracy: 0.2000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.31153, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 2.3042 - accuracy: 0.1445 - val_loss: 2.2711 - val_accuracy: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.31153 to 2.27113, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 2.2021 - accuracy: 0.2522 - val_loss: 2.2249 - val_accuracy: 0.4600\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.27113 to 2.22494, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 2.0422 - accuracy: 0.4771 - val_loss: 2.0383 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.22494 to 2.03830, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 1.7183 - accuracy: 0.5151 - val_loss: 1.6786 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.03830 to 1.67856, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 1.3198 - accuracy: 0.5701 - val_loss: 1.2618 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.67856 to 1.26176, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 1.1120 - accuracy: 0.6434 - val_loss: 0.9234 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26176 to 0.92342, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.8641 - accuracy: 0.7177 - val_loss: 0.7704 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.92342 to 0.77044, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.6156 - accuracy: 0.8478 - val_loss: 0.4044 - val_accuracy: 0.9400\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.77044 to 0.40437, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 0.3310 - accuracy: 0.9505 - val_loss: 0.2355 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.40437 to 0.23552, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 1s 125ms/step - loss: 0.2367 - accuracy: 0.9475 - val_loss: 0.1515 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23552 to 0.15153, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.1815 - accuracy: 0.9572 - val_loss: 0.1209 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.15153 to 0.12085, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.1290 - accuracy: 0.9886 - val_loss: 0.0609 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12085 to 0.06089, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.0789 - accuracy: 0.9817 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.06089 to 0.02977, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.0650 - accuracy: 0.9899 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02977 to 0.02163, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.0466 - accuracy: 0.9978 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02163 to 0.01890, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.0468 - accuracy: 0.9947 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01890 to 0.00768, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00768 to 0.00474, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.0436 - accuracy: 0.9869 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00474\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 0.0239 - accuracy: 0.9921 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00474\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.0512 - accuracy: 0.9869 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00474 to 0.00394, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.0313 - accuracy: 0.9869 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00394 to 0.00294, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00294 to 0.00242, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 0.0234 - accuracy: 0.9965 - val_loss: 0.0021 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss improved from 0.00242 to 0.00206, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.0141 - accuracy: 0.9947 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00206\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00206\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00206 to 0.00152, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 8.8455e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00152 to 0.00088, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 1s 125ms/step - loss: 0.0180 - accuracy: 0.9930 - val_loss: 6.2274e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00088 to 0.00062, saving model to ./upgrade_model/upgrade_multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./upgrade_model/upgrade_multi_img_classification.model/assets\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00062\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.0267 - accuracy: 0.9921 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00062\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00062\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00062\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00062\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.0178 - accuracy: 0.9895 - val_loss: 9.4722e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00062\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "model = load_model('./model/multi_img_classification.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      " * Restarting with fsevents reloader\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopre/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "import numpy as np\n",
    "import os, glob, numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods = ['GET', 'POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        X=[]\n",
    "        f = request.files['image']\n",
    "        #저장할 경로 + 파일명\n",
    "        f.save(f.filename)\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w,image_h))\n",
    "        data = np.asarray(img)\n",
    "        X.append(data)\n",
    "        X = np.array(X)\n",
    "        model = load_model('./model/multi_img_classification.model')\n",
    "        prediction = model.predict(X)\n",
    "        np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "        pre_ans = prediction.argmax()\n",
    "\n",
    "        print(pre_ans)\n",
    "        pre_ans_str = ''\n",
    "        if pre_ans == 0: pre_ans_str = \"김치찌개\"\n",
    "        elif pre_ans == 1: pre_ans_str = \"라면\"\n",
    "        elif pre_ans == 2: pre_ans_str = \"양념게장\"\n",
    "        else: pre_ans_str = \"제육볶음\"\n",
    "        if pre_ans[0] >= 0.8: print(\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "        if pre_ans[1] >= 0.8: print(\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "        if pre_ans[2] >= 0.8: print(\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "        if pre_ans[3] >= 0.8: print(\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "        return pre_ans_str\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      " * Restarting with fsevents reloader\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopre/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from flask import Flask, request\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods = ['GET', 'POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        class_names = ['김치찌개','양념게장', '라면']\n",
    "        model = keras.models.load_model('model.h5')\n",
    "        f = request.files['image']\n",
    "        #저장할 경로 + 파일명\n",
    "        f.save(f.filename)\n",
    "        img = keras.preprocessing.image.load_img(\n",
    "            f.filename, target_size=(150, 150)\n",
    "        )\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "        predictions = model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "        return class_names[np.argmax(score)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
